# -*- coding: utf-8 -*-
"""Diff ML models for Movie dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ik5DvvAF0l7NmZYDzNOuRQkPIfElXIMG

**Movie Review - Sentiment Analysis - Classify the sentiment of sentences from the Rotten Tomatoes dataset**

**Developing Machine Learning Models using TF - IDF as feature extraction / representation**

**Load the required libraries**
"""

import pandas as pd
import matplotlib
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import nltk

!pip install scikit-plot

"""**Load Trainig and testing dataset**"""

df = pd.read_csv("/content/train.tsv", sep="\t")

df_test = pd.read_csv("/content/test.tsv", sep="\t")

"""**Preview of datasets**"""

df.head()

df_test.head()

"""**Creating the TF - IDF vectorizer**"""

from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import TweetTokenizer

tokenizer = TweetTokenizer()

vectorizer = TfidfVectorizer(ngram_range=(1, 3), tokenizer=tokenizer.tokenize)
full_text = list(df['Phrase'].values) + list(df_test['Phrase'].values)

vectorizer.fit(full_text)
train_vectorized = vectorizer.transform(df['Phrase'])
test_vectorized = vectorizer.transform(df_test['Phrase'])

print("Train set dimensions after applying TF-IDF vectorizer:")
np.shape(train_vectorized)

X = train_vectorized
y = df.Sentiment.values


from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report
import time

# split the data into 80:20 for training and validating.
xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify=y, random_state=42, test_size=0.2, shuffle=True)

ml_default_performance_metrics_df = pd.DataFrame(columns=['accuracy','F1-score','training-time'], index=['LogisticRegression', 'DecisionTreeClassifier', 'ExtraTreesClassifier', 'RandomForestClassifier', 'LinearSVC', 'BernoulliNB', 'MultinomialNB', 'KNeighborsClassifier'])

"""**Machine learning techniques used**

**1. Multinomial Logistic Regression**
"""

from sklearn.linear_model import LogisticRegression

start_time = time.time()

clf_logistic_regression = LogisticRegression(multi_class='ovr', solver='sag', random_state=42)
clf_logistic_regression.fit(xtrain, ytrain)
predictions = clf_logistic_regression.predict(xvalid)

print(classification_report(yvalid, predictions))

print()
print("accuracy_score", accuracy_score(yvalid, predictions))

print()
print("Weighted Averaged validation metrics")
print("precision_score", precision_score(yvalid, predictions, average='weighted'))
print("recall_score", recall_score(yvalid, predictions, average='weighted'))
print("f1_score", f1_score(yvalid, predictions, average='weighted'))

print()
from sklearn.metrics import confusion_matrix
import scikitplot as skplt
sns.set(rc={'figure.figsize':(8,8)})
skplt.metrics.plot_confusion_matrix(yvalid, predictions)


ml_default_performance_metrics_df.loc['LogisticRegression']['training-time'] = time.time() - start_time
ml_default_performance_metrics_df.loc['LogisticRegression']['accuracy'] = accuracy_score(yvalid, predictions)
ml_default_performance_metrics_df.loc['LogisticRegression']['F1-score'] = f1_score(yvalid, predictions, average='micro')


print()
print("elapsed time in seconds: ", time.time() - start_time)
print()

"""**Multinomial Logistic Regression results in a good accuracy of 63% and F1 score of 59%**

**2. Decision Tree Classifier**
"""

from sklearn.tree import DecisionTreeClassifier

start_time = time.time()
    
print()
print("Evaluation of DecisionTreeClassifier, with train-test split:")

clf_DecisionTreeClassifier = DecisionTreeClassifier(random_state=42)
clf_DecisionTreeClassifier.fit(xtrain, ytrain)
predictions = clf_DecisionTreeClassifier.predict(xvalid)
print(classification_report(yvalid, predictions))

print()
print("accuracy_score", accuracy_score(yvalid, predictions))

print()
print("Weighted Averaged validation metrics")
print("precision_score", precision_score(yvalid, predictions, average='weighted'))
print("recall_score", recall_score(yvalid, predictions, average='weighted'))
print("f1_score", f1_score(yvalid, predictions, average='weighted'))

print()
from sklearn.metrics import confusion_matrix
import scikitplot as skplt
sns.set(rc={'figure.figsize':(8,8)})
skplt.metrics.plot_confusion_matrix(yvalid, predictions)

ml_default_performance_metrics_df.loc['DecisionTreeClassifier']['training-time'] = time.time() - start_time
ml_default_performance_metrics_df.loc['DecisionTreeClassifier']['accuracy'] = accuracy_score(yvalid, predictions)
ml_default_performance_metrics_df.loc['DecisionTreeClassifier']['F1-score'] = f1_score(yvalid, predictions, average='weighted')

print()
print("elapsed time in seconds: ", time.time() - start_time)
print()

"""**Decision tree classifier results in less accuracy of  55% and F1 score is 53%**

**3. Extra Trees Classifier**

ExtraTrees implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting
"""

from sklearn.ensemble import ExtraTreesClassifier

start_time = time.time()
    
print()
print("Evaluation of ExtraTreesClassifier with train-test split:")

clf_ExtraTreesClassifier = ExtraTreesClassifier(n_estimators=10, random_state=42)
clf_ExtraTreesClassifier.fit(xtrain, ytrain)
predictions = clf_ExtraTreesClassifier.predict(xvalid)
print(classification_report(yvalid, predictions))

print()
print("accuracy_score", accuracy_score(yvalid, predictions))

print()
print("Weighted Averaged validation metrics")
print("precision_score", precision_score(yvalid, predictions, average='weighted'))
print("recall_score", recall_score(yvalid, predictions, average='weighted'))
print("f1_score", f1_score(yvalid, predictions, average='weighted'))


ml_default_performance_metrics_df.loc['ExtraTreesClassifier']['training-time'] = time.time() - start_time
ml_default_performance_metrics_df.loc['ExtraTreesClassifier']['accuracy'] = accuracy_score(yvalid, predictions)
ml_default_performance_metrics_df.loc['ExtraTreesClassifier']['F1-score'] = f1_score(yvalid, predictions, average='weighted')

print()
from sklearn.metrics import confusion_matrix
import scikitplot as skplt
sns.set(rc={'figure.figsize':(8,8)})
skplt.metrics.plot_confusion_matrix(yvalid, predictions)

print()
print("elapsed time in seconds: ", time.time() - start_time)
print()

"""**Extra Trees Classifier results in a good accuracy of  63%and F1 score of 61%**

**4. Random Forest Classifier**
"""

from sklearn.ensemble import RandomForestClassifier

start_time = time.time()
    
print()
print("Evaluation of RandomForestClassifier with train-test split:")

clf_RandomForestClassifier = RandomForestClassifier(n_estimators = 10, random_state=42)
clf_RandomForestClassifier.fit(xtrain, ytrain)
predictions = clf_RandomForestClassifier.predict(xvalid)
print(classification_report(yvalid, predictions))

print()
print("accuracy_score", accuracy_score(yvalid, predictions))

print()
print("Weighted Averaged validation metrics")
print("precision_score", precision_score(yvalid, predictions, average='weighted'))
print("recall_score", recall_score(yvalid, predictions, average='weighted'))
print("f1_score", f1_score(yvalid, predictions, average='weighted'))


ml_default_performance_metrics_df.loc['RandomForestClassifier']['training-time'] = time.time() - start_time
ml_default_performance_metrics_df.loc['RandomForestClassifier']['accuracy'] = accuracy_score(yvalid, predictions)
ml_default_performance_metrics_df.loc['RandomForestClassifier']['F1-score'] = f1_score(yvalid, predictions, average='weighted')

print()
from sklearn.metrics import confusion_matrix
import scikitplot as skplt
sns.set(rc={'figure.figsize':(8,8)})
skplt.metrics.plot_confusion_matrix(yvalid, predictions)


print()
print("elapsed time in seconds: ", time.time() - start_time)
print()

"""**Random Forest Classifier also  results in a good accuracy of  62% and F1 score of 60%**

**5. Linear SVC**
"""

from sklearn.svm import LinearSVC

start_time = time.time()
    
print()
print("Evaluation of LinearSVC, multi_class='ovr', with train-test split:")

clf_LinearSVC = LinearSVC(multi_class='ovr', random_state=42)
clf_LinearSVC.fit(xtrain, ytrain)
predictions = clf_LinearSVC.predict(xvalid)
print(classification_report(yvalid, predictions))

print()
print("accuracy_score", accuracy_score(yvalid, predictions))

print()
print("Weighted Averaged validation metrics")
print("precision_score", precision_score(yvalid, predictions, average='weighted'))
print("recall_score", recall_score(yvalid, predictions, average='weighted'))
print("f1_score", f1_score(yvalid, predictions, average='weighted'))


ml_default_performance_metrics_df.loc['LinearSVC']['training-time'] = time.time() - start_time
ml_default_performance_metrics_df.loc['LinearSVC']['accuracy'] = accuracy_score(yvalid, predictions)
ml_default_performance_metrics_df.loc['LinearSVC']['F1-score'] = f1_score(yvalid, predictions, average='weighted')

print()
from sklearn.metrics import confusion_matrix
import scikitplot as skplt
sns.set(rc={'figure.figsize':(8,8)})
skplt.metrics.plot_confusion_matrix(yvalid, predictions)


print()
print("elapsed time in seconds: ", time.time() - start_time)
print()

"""**Linear SVC classifier exhibits great accuracy so far 65% and F1-score 64%**

**6. Bernoulli NB**
"""

from sklearn.naive_bayes import BernoulliNB

start_time = time.time()
    
print()
print("Evaluation of BernoulliNB with train-test split:")

clf_BernoulliNB = BernoulliNB()
clf_BernoulliNB.fit(xtrain, ytrain)
predictions = clf_BernoulliNB.predict(xvalid)
print(classification_report(yvalid, predictions))

print()
print("accuracy_score", accuracy_score(yvalid, predictions))

print()
print("Weighted Averaged validation metrics")
print("precision_score", precision_score(yvalid, predictions, average='weighted'))
print("recall_score", recall_score(yvalid, predictions, average='weighted'))
print("f1_score", f1_score(yvalid, predictions, average='weighted'))


ml_default_performance_metrics_df.loc['BernoulliNB']['training-time'] = time.time() - start_time
ml_default_performance_metrics_df.loc['BernoulliNB']['accuracy'] = accuracy_score(yvalid, predictions)
ml_default_performance_metrics_df.loc['BernoulliNB']['F1-score'] = f1_score(yvalid, predictions, average='weighted')

print()
from sklearn.metrics import confusion_matrix
import scikitplot as skplt
sns.set(rc={'figure.figsize':(8,8)})
skplt.metrics.plot_confusion_matrix(yvalid, predictions)


print()
print("elapsed time in seconds: ", time.time() - start_time)
print()

"""**Bernoulli NB results in a accuracy of 60% and F1 score 56%**

**7. Multinomial NB**
"""

from sklearn.naive_bayes import MultinomialNB

start_time = time.time()
    
print()
print("Evaluation of MultinomialNB with train-test split:")

clf_MultinomialNB = MultinomialNB()
clf_MultinomialNB.fit(xtrain, ytrain)
predictions = clf_MultinomialNB.predict(xvalid)
print(classification_report(yvalid, predictions))

print()
print("accuracy_score", accuracy_score(yvalid, predictions))

print()
print("Weighted Averaged validation metrics")
print("precision_score", precision_score(yvalid, predictions, average='weighted'))
print("recall_score", recall_score(yvalid, predictions, average='weighted'))
print("f1_score", f1_score(yvalid, predictions, average='weighted'))


ml_default_performance_metrics_df.loc['MultinomialNB']['training-time'] = time.time() - start_time
ml_default_performance_metrics_df.loc['MultinomialNB']['accuracy'] = accuracy_score(yvalid, predictions)
ml_default_performance_metrics_df.loc['MultinomialNB']['F1-score'] = f1_score(yvalid, predictions, average='weighted')

print()
from sklearn.metrics import confusion_matrix
import scikitplot as skplt
sns.set(rc={'figure.figsize':(8,8)})
skplt.metrics.plot_confusion_matrix(yvalid, predictions)


print()
print("elapsed time in seconds: ", time.time() - start_time)
print()

"""**Multinomail NB results in a accuracy of 61% and F1 score 55%**

**8. KNeighbors Classifier**
"""

from sklearn.neighbors import KNeighborsClassifier

start_time = time.time()

clf_knn = KNeighborsClassifier()
clf_knn.fit(xtrain, ytrain)
predictions = clf_knn.predict(xvalid)

print(classification_report(yvalid, predictions))

print()
print("accuracy_score", accuracy_score(yvalid, predictions))

print()
print("Weighted Averaged validation metrics")
print("precision_score", precision_score(yvalid, predictions, average='weighted'))
print("recall_score", recall_score(yvalid, predictions, average='weighted'))
print("f1_score", f1_score(yvalid, predictions, average='weighted'))


ml_default_performance_metrics_df.loc['KNeighborsClassifier']['training-time'] = time.time() - start_time
ml_default_performance_metrics_df.loc['KNeighborsClassifier']['accuracy'] = accuracy_score(yvalid, predictions)
ml_default_performance_metrics_df.loc['KNeighborsClassifier']['F1-score'] = f1_score(yvalid, predictions, average='weighted')

print()
from sklearn.metrics import confusion_matrix
import scikitplot as skplt
sns.set(rc={'figure.figsize':(8,8)})
skplt.metrics.plot_confusion_matrix(yvalid, predictions)


print()
print("elapsed time in seconds: ", time.time() - start_time)
print()

"""**K neighbours results in a accuracy of 59% and F1 score 59%**

**Summarizing ML Classifiers based on their accuracy with TF - IDF as Feature Extraction**
"""

ml_default_performance_metrics_df.sort_values(by="accuracy", ascending=False)

sns.set(rc={'figure.figsize':(15.27,6.27)})
ml_default_performance_metrics_df.sort_values(by="accuracy", ascending=False).accuracy.plot(kind="bar")

"""**Summarizing ML Classifiers based on their F1-score with TF - IDF as Feature Extraction**"""

ml_default_performance_metrics_df.sort_values(by="F1-score", ascending=True)

sns.set(rc={'figure.figsize':(15.27,6.27)})
ml_default_performance_metrics_df.sort_values(by="F1-score", ascending=False)["F1-score"].plot(kind="bar")

"""**Summarizing ML Classifiers based on their training fitting time with TF - IDF as Feature Extraction**"""

ml_default_performance_metrics_df.sort_values(by="training-time", ascending=True)

sns.set(rc={'figure.figsize':(15.27,6.27)})
ml_default_performance_metrics_df.sort_values(by="training-time", ascending=True)["training-time"].plot(kind="bar")

"""**Top 4 models with High accuracy**

1. LinearSVC (SVM Classifier)
2. Logistic Regression Classifier
3. ExtraTreesClassifier
4. RandomForestClassfier

**Ensemble Top 4 Models over the validation Set**
"""

predictions_linear_svc = clf_LinearSVC.predict(xvalid)
predictions_extra_trees = clf_ExtraTreesClassifier.predict(xvalid)
predictions_logistic_regression = clf_logistic_regression.predict(xvalid)
predictions_random_forest = clf_RandomForestClassifier.predict(xvalid)

ensemble_all_ml_default_pred_df = pd.DataFrame({
    'linear_svc':predictions_linear_svc,
    'logistic_regression':predictions_logistic_regression,
    'extra_trees':predictions_extra_trees,
    'random_forest':predictions_random_forest})


# ensemble classes' predictions over the validation set
pred_mode = ensemble_all_ml_default_pred_df.agg('mode',axis=1)[0].values

print(classification_report(yvalid, pred_mode))

print()
print("accuracy_score", accuracy_score(yvalid, pred_mode))

print()
print("Weighted Averaged validation metrics")
print("precision_score", precision_score(yvalid, pred_mode, average='weighted'))
print("recall_score", recall_score(yvalid, pred_mode, average='weighted'))
print("f1_score", f1_score(yvalid, pred_mode, average='weighted'))

print()
from sklearn.metrics import confusion_matrix
import scikitplot as skplt
sns.set(rc={'figure.figsize':(8,8)})
skplt.metrics.plot_confusion_matrix(yvalid, pred_mode)

"""**Producing Predictions for the Test Set for the top 4 best performed ML models**"""

df_test = pd.read_csv('/content/test.tsv', sep="\t")

test_vectorized = vectorizer.transform(df_test['Phrase'])

predictions_default_linear_svc = clf_LinearSVC.predict(test_vectorized)
submission = pd.DataFrame()
submission['PhraseId'] = df_test.PhraseId
submission['Sentiment'] = predictions_default_linear_svc
#submission['Sentiment'] = submission.Sentiment.astype(int)
submission.to_csv('submission_linear_svc.csv',index=False)

predictions_default_logistic_regression = clf_logistic_regression.predict(test_vectorized)
submission = pd.DataFrame()
submission['PhraseId'] = df_test.PhraseId
submission['Sentiment'] = predictions_default_logistic_regression
#submission['Sentiment'] = submission.Sentiment.astype(int)
submission.to_csv('submission_logistic_regression.csv',index=False)

predictions_default_extra_trees = clf_ExtraTreesClassifier.predict(test_vectorized)
submission = pd.DataFrame()
submission['PhraseId'] = df_test.PhraseId
submission['Sentiment'] = predictions_default_extra_trees
#submission['Sentiment'] = submission.Sentiment.astype(int)
submission.to_csv('submission_extra_trees.csv',index=False)

predictions_default_random_forest = clf_RandomForestClassifier.predict(test_vectorized)
submission = pd.DataFrame()
submission['PhraseId'] = df_test.PhraseId
submission['Sentiment'] = predictions_default_random_forest
#submission['Sentiment'] = submission.Sentiment.astype(int)
submission.to_csv('submission_random_forest.csv',index=False)

"""**Tuning the top 4 best performed ML Models**"""

ml_tuning_performance_metrics_df = pd.DataFrame(columns=['accuracy','F1-score','training-time'], index=['LogisticRegression', 'ExtraTreesClassifier', 'RandomForestClassifier', 'LinearSVC'])

"""**LinearSVC tuning**"""

from sklearn.model_selection import GridSearchCV
from sklearn.svm import LinearSVC
import time

start_time = time.time()
print("Linear SVC grid Search:")

clf_linear_svc_grid = LinearSVC(multi_class='ovr', random_state=42)

X = train_vectorized
y = df.Sentiment.values

xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify=y, random_state=42, test_size=0.2, shuffle=True)

gsc_linear_svc = GridSearchCV(
    estimator=clf_linear_svc_grid,
    param_grid={
        'C': [10e-1, 10e-2, 1, 10],
        'tol': [10e-5, 10e-4, 10e-3, 10e-2, 10e-1, 10e-0, 10e+1],
    },
    scoring='accuracy',
    cv=2
)

grid_result_linear_svc = gsc_linear_svc.fit(xtrain, ytrain)

print()
print("ExtraTreesClassifier best parameters and score")
print("Best score: %f using %s" % (grid_result_linear_svc.best_score_, grid_result_linear_svc.best_params_))

prediction_tuned_linear_svc = gsc_linear_svc.predict(xvalid)
print("accuracy score after tuning:", accuracy_score(yvalid, prediction_tuned_linear_svc))

ml_tuning_performance_metrics_df.loc['LinearSVC']['training-time'] = time.time() - start_time
ml_tuning_performance_metrics_df.loc['LinearSVC']['accuracy'] = accuracy_score(yvalid, prediction_tuned_linear_svc)
ml_tuning_performance_metrics_df.loc['LinearSVC']['F1-score'] = f1_score(yvalid, prediction_tuned_linear_svc, average='macro')

print()
print("elapsed time in seconds: ", time.time() - start_time)
print()

"""**LinearSVC tuning evaluation**"""

predictions_tuned_linear_svc = grid_result_linear_svc.predict(xvalid)

print(classification_report(yvalid, predictions_tuned_linear_svc))

print()
print("accuracy_score", accuracy_score(yvalid, predictions_tuned_linear_svc))

print()
print("Weighted Averaged validation metrics")
print("precision_score", precision_score(yvalid, predictions_tuned_linear_svc, average='weighted'))
print("recall_score", recall_score(yvalid, predictions_tuned_linear_svc, average='weighted'))
print("f1_score", f1_score(yvalid, predictions_tuned_linear_svc, average='weighted'))


print()
from sklearn.metrics import confusion_matrix
import scikitplot as skplt
sns.set(rc={'figure.figsize':(8,8)})
skplt.metrics.plot_confusion_matrix(yvalid, predictions_tuned_linear_svc)

"""**In Linear SVC tuning there is no improvement in accuracy and F1 score**

**Logistic Regression Tuning**
"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
import time

start_time = time.time()
print("LogisticRegression grid Search:")

clf_logistic_regression_grid = LogisticRegression(multi_class = 'ovr', random_state=42)

X = train_vectorized
y = df.Sentiment.values

xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify=y, random_state=42, test_size=0.2, shuffle=True)

gsc_logistic_regression = GridSearchCV(
    estimator=clf_logistic_regression_grid,
    param_grid={
        'solver': ['newton-cg', 'sag', 'saga'],
        'C': [10e-1, 10e-2, 1, 10],
        'tol': [10e-4, 10e-3, 10e-2, 10e-1, 10e-0],
    },
    scoring='accuracy',
    cv=2
)

grid_result_logistic_regression = gsc_logistic_regression.fit(xtrain, ytrain)

print()
print("LogisticRegression best parameters and score")
print("Best score: %f using %s" % (grid_result_logistic_regression.best_score_, grid_result_logistic_regression.best_params_))

prediction_tuned_logistic_regression = grid_result_logistic_regression.predict(xvalid)
print("accuracy score after tuning:", accuracy_score(yvalid, prediction_tuned_logistic_regression))

ml_tuning_performance_metrics_df.loc['LogisticRegression']['training-time'] = time.time() - start_time
ml_tuning_performance_metrics_df.loc['LogisticRegression']['accuracy'] = accuracy_score(yvalid, prediction_tuned_logistic_regression)
ml_tuning_performance_metrics_df.loc['LogisticRegression']['F1-score'] = f1_score(yvalid, prediction_tuned_logistic_regression, average='macro')

print()
print("elapsed time in seconds: ", time.time() - start_time)
print()

"""**LogisticRegression tuning evaluation**"""

predictions_tuned_logistic_regression = grid_result_logistic_regression.predict(xvalid)

print(classification_report(yvalid, predictions_tuned_logistic_regression))

print()
print("accuracy_score", accuracy_score(yvalid, predictions_tuned_logistic_regression))

print()
print("Weighted Averaged validation metrics")
print("precision_score", precision_score(yvalid, predictions_tuned_logistic_regression, average='weighted'))
print("recall_score", recall_score(yvalid, predictions_tuned_logistic_regression, average='weighted'))
print("f1_score", f1_score(yvalid, predictions_tuned_logistic_regression, average='weighted'))


print()
from sklearn.metrics import confusion_matrix
import scikitplot as skplt
sns.set(rc={'figure.figsize':(8,8)})
skplt.metrics.plot_confusion_matrix(yvalid, predictions_tuned_logistic_regression)

"""**In Logistic Regression tuning also we cannot see any improvement in accuracy and F1 score**

**ExtraTrees Tuning**
"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import ExtraTreesClassifier
import time

start_time = time.time()
print("ExtraTreesClassifier grid Search:")

clf_extra_trees_grid = ExtraTreesClassifier(random_state=42)

X = train_vectorized
y = df.Sentiment.values

xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify=y, random_state=42, test_size=0.2, shuffle=True)

gsc_extra_trees = GridSearchCV(
    estimator=clf_extra_trees_grid,
    param_grid={
        'n_estimators': [2, 4, 6, 8, 10, 12]
    },
    scoring='accuracy',
    cv=2
)

grid_result_extra_trees = gsc_extra_trees.fit(xtrain, ytrain)

print()
print("ExtraTreesClassifier best parameters and score")
print("Best score: %f using %s" % (grid_result_extra_trees.best_score_, grid_result_extra_trees.best_params_))

prediction_tuned_extra_trees = grid_result_extra_trees.predict(xvalid)
print("accuracy score after tuning:", accuracy_score(yvalid, prediction_tuned_extra_trees))

ml_tuning_performance_metrics_df.loc['ExtraTreesClassifier']['training-time'] = time.time() - start_time
ml_tuning_performance_metrics_df.loc['ExtraTreesClassifier']['accuracy'] = accuracy_score(yvalid, prediction_tuned_extra_trees)
ml_tuning_performance_metrics_df.loc['ExtraTreesClassifier']['F1-score'] = f1_score(yvalid, prediction_tuned_extra_trees, average='macro')

print()
print("elapsed time in seconds: ", time.time() - start_time)
print()

"""**ExtraTrees tuning evaluation**"""

predictions_tuned_extra_trees = grid_result_extra_trees.predict(xvalid)

print(classification_report(yvalid, predictions_tuned_extra_trees))

print()
print("accuracy_score", accuracy_score(yvalid, predictions_tuned_extra_trees))

print()
print("Weighted Averaged validation metrics")
print("precision_score", precision_score(yvalid, predictions_tuned_extra_trees, average='weighted'))
print("recall_score", recall_score(yvalid, predictions_tuned_extra_trees, average='weighted'))
print("f1_score", f1_score(yvalid, predictions_tuned_extra_trees, average='weighted'))


print()
from sklearn.metrics import confusion_matrix
import scikitplot as skplt
sns.set(rc={'figure.figsize':(8,8)})
skplt.metrics.plot_confusion_matrix(yvalid, predictions_tuned_extra_trees)

"""**Again in the ExtraTrees tuning the accuracy and F1 score has no improvement**

**RandomForestClassifier Tuning**
"""

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.ensemble import RandomForestClassifier

start_time = time.time()
print("RandomForestClassifier grid Search:")

clf_random_forest = RandomForestClassifier(random_state=42)

X = train_vectorized
y = df.Sentiment.values

xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify=y, random_state=42, test_size=0.2, shuffle=True)

gsc_random_forest = GridSearchCV(
    estimator=clf_random_forest,
    param_grid={
        'n_estimators': [2, 4, 6, 8, 10, 12]
    },
    scoring='accuracy',
    cv=2
)

grid_result_random_forest = gsc_random_forest.fit(xtrain, ytrain)

print()
print("RandomForest best parameters and score")
print("Best: %f using %s" % (grid_result_random_forest.best_score_, grid_result_random_forest.best_params_))

prediction_tuned_random_forest = grid_result_random_forest.predict(xvalid)
print("accuracy score after tuning:", accuracy_score(yvalid, prediction_tuned_random_forest))

ml_tuning_performance_metrics_df.loc['RandomForestClassifier']['training-time'] = time.time() - start_time
ml_tuning_performance_metrics_df.loc['RandomForestClassifier']['accuracy'] = accuracy_score(yvalid, prediction_tuned_random_forest)
ml_tuning_performance_metrics_df.loc['RandomForestClassifier']['F1-score'] = f1_score(yvalid, prediction_tuned_random_forest, average='macro')

print()
print("elapsed time in seconds: ", time.time() - start_time)
print()

"""**RandomForest tuning evaluation**"""

predictions_tuned_random_forest = grid_result_random_forest.predict(xvalid)

print(classification_report(yvalid, predictions_tuned_random_forest))

print()
print("accuracy_score", accuracy_score(yvalid, predictions_tuned_random_forest))

print()
print("Weighted Averaged validation metrics")
print("precision_score", precision_score(yvalid, predictions_tuned_random_forest, average='weighted'))
print("recall_score", recall_score(yvalid, predictions_tuned_random_forest, average='weighted'))
print("f1_score", f1_score(yvalid, predictions_tuned_random_forest, average='weighted'))


print()
from sklearn.metrics import confusion_matrix
import scikitplot as skplt
sns.set(rc={'figure.figsize':(8,8)})
skplt.metrics.plot_confusion_matrix(yvalid, predictions_tuned_random_forest)

"""**Same again in the Random Tree tuning the accuracy and F1 score has no improvement**

**Comparing the accuracy and F1-score between default and tuned parameters in ML models**
"""

ml_tuning_performance_metrics_df.sort_values(by="accuracy", ascending=False)

"""**Although there are some improvements in accuracy and F1-score tuning and in fact provided some minor advantages to out models based on the default and after tuning.**"""



"""**Ensemble The top 4 Tuned Models over the validation Set**"""

predictions_tuned_linear_svc = grid_result_linear_svc.predict(xvalid)
predictions_tuned_logistic_regression = grid_result_logistic_regression.predict(xvalid)
predictions_tuned_extra_trees = grid_result_extra_trees.predict(xvalid)
predictions_tuned_random_forest = grid_result_random_forest.predict(xvalid)

ensemble_all_ml_tuned_pred_df = pd.DataFrame({
    'linear_svc':predictions_tuned_linear_svc,
    'logistic_regression':predictions_tuned_logistic_regression,
    'extra_trees':predictions_tuned_extra_trees,
    'random_forest':predictions_tuned_random_forest})

pred_tuned_mode = ensemble_all_ml_tuned_pred_df.agg('mode',axis=1)[0].values

print(classification_report(yvalid, pred_tuned_mode))

print()
print("accuracy_score", accuracy_score(yvalid, pred_tuned_mode))

print()
print("Weighted Averaged validation metrics")
print("precision_score", precision_score(yvalid, pred_tuned_mode, average='weighted'))
print("recall_score", recall_score(yvalid, pred_tuned_mode, average='weighted'))
print("f1_score", f1_score(yvalid, pred_tuned_mode, average='weighted'))


print()
from sklearn.metrics import confusion_matrix
import scikitplot as skplt
sns.set(rc={'figure.figsize':(8,8)})
skplt.metrics.plot_confusion_matrix(yvalid, pred_tuned_mode)

"""**ML Predictions with Test Set**"""

df_test = pd.read_csv("/content/test.tsv", sep="\t")

test_vectorized = vectorizer.transform(df_test['Phrase'])

predictions_tuned_test_linear_svc = grid_result_linear_svc.predict(test_vectorized)
submission = pd.DataFrame()
submission['PhraseId'] = df_test.PhraseId
submission['Sentiment'] = predictions_tuned_test_linear_svc
#submission['Sentiment'] = submission.Sentiment.astype(int)
submission.to_csv('submission_tuned_linear_svc.csv',index=False)

predictions_tuned_test_logistic_regression = grid_result_logistic_regression.predict(test_vectorized)
submission = pd.DataFrame()
submission['PhraseId'] = df_test.PhraseId
submission['Sentiment'] = predictions_tuned_test_logistic_regression
#submission['Sentiment'] = submission.Sentiment.astype(int)
submission.to_csv('submission_tuned_logistic_regression.csv',index=False)

predictions_tuned_test_extra_trees = grid_result_extra_trees.predict(test_vectorized)
submission = pd.DataFrame()
submission['PhraseId'] = df_test.PhraseId
submission['Sentiment'] = predictions_tuned_test_extra_trees
#submission['Sentiment'] = submission.Sentiment.astype(int)
submission.to_csv('submission_tuned_extra_trees.csv',index=False)

predictions_tuned_test_random_forest = grid_result_random_forest.predict(test_vectorized)
submission = pd.DataFrame()
submission['PhraseId'] = df_test.PhraseId
submission['Sentiment'] = predictions_tuned_test_random_forest
#submission['Sentiment'] = submission.Sentiment.astype(int)
submission.to_csv('submission_tuned_random_forest.csv',index=False)

"""**Ensemble Predictions after Tuning from top 4 best performed ML Techniques after tuning**"""

ensemble_all_ml_tuned_pred_test_df = pd.DataFrame({
    'linear_svc':predictions_tuned_test_linear_svc,
    'logistic_regression':predictions_tuned_test_logistic_regression,
    'extra_trees':predictions_tuned_test_extra_trees,
    'random_forest':predictions_tuned_test_random_forest})


pred_tuned_test_mode = ensemble_all_ml_tuned_pred_test_df.agg('mode',axis=1)[0].values
submission = pd.DataFrame()
submission['PhraseId'] = df_test.PhraseId
submission['Sentiment'] = pred_tuned_test_mode
submission['Sentiment'] = submission.Sentiment.astype(int)
submission.to_csv('submission_ml_tuned_ensemble.csv',index=False)

